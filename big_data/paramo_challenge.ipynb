{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Session, Context and Dummy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName(\"paramo_challenge\").getOrCreate()\n",
    "sc = spark.sparkContext.getOrCreate()\n",
    "\n",
    "employee_columns = [\"emp_no\", \"birth_date\", \"first_name\", \"last_name\", \"gender\", \"hire_date\"]\n",
    "employee_body = [[\"10001\",\"1953-09-02\",\"Georgi\",\"Facello\",\"M\",\"1986-06-26\"],\n",
    "    [\"10002\",\"1964-06-02\",\"Bezalel\",\"Simmel\",\"F\",\"1985-11-21\"],\n",
    "    [\"10003\",\"1959-12-03\",\"Parto\",\"Bamford\",\"M\",\"1986-08-28\"],\n",
    "    [\"10004\",\"1954-05-01\",\"Chirstian\",\"Koblick\",\"M\",\"1986-12-01\"],\n",
    "    [\"10005\",\"1955-01-21\",\"Kyoichi\",\"Maliniak\",\"M\",\"1989-09-12\"]] \n",
    "\n",
    "job_columns = [\"emp_no\", \"title\", \"from_date\" , \"to_date\"] \n",
    "job_body = [[\"10001\",\"Senior Engineer\",\"1986-06-26\",\"9999-01-01\"],\n",
    "    [\"10002\",\"Staff\",\"1996-08-03\",\"9999-01-01\"],\n",
    "    [\"10003\",\"Senior Engineer\",\"1995-12-03\",\"9999-01-01\"],\n",
    "    [\"10004\",\"Senior Engineer\",\"1995-12-01\",\"9999-01-01\"],\n",
    "    [\"10005\",\"Senior Staff\",\"1996-09-12\",\"9999-01-01\"]] \n",
    "\n",
    "salary_columns = [\"emp_no\", \"salary\", \"from_date\" , \"to_date\"] \n",
    "salary_body = [[\"10001\",\"66074\",\"1988-06-25\",\"1989-06-25\"], \n",
    "    [\"10001\",\"62102\",\"1987-06-26\",\"1988-06-25\"],\n",
    "    [\"10001\",\"60117\",\"1986-06-26\",\"1987-06-26\"], \n",
    "    [\"10002\",\"72527\",\"2001-08-02\",\"9999-01-01\"],\n",
    "    [\"10002\",\"71963\",\"2000-08-02\",\"2001-08-02\"], \n",
    "    [\"10002\",\"69366\",\"1999-08-03\",\"2000-08-02\"],\n",
    "    [\"10003\",\"43311\",\"2001-12-01\",\"9999-01-01\"], \n",
    "    [\"10003\",\"43699\",\"2000-12-01\",\"2001-12-01\"],\n",
    "    [\"10003\",\"43478\",\"1999-12-02\",\"2000-12-01\"],\n",
    "    [\"10004\",\"74057\",\"2001-11-27\",\"9999-01-01\"], \n",
    "    [\"10004\",\"70698\",\"2000-11-27\",\"2001-11-27\"],\n",
    "    [\"10004\",\"69722\",\"1999-11-28\",\"2000-11-27\"],\n",
    "    [\"10005\",\"94692\",\"2001-09-09\",\"9999-01-01\"], \n",
    "    [\"10005\",\"91453\",\"2000-09-09\",\"2001-09-09\"],\n",
    "    [\"10005\",\"90531\",\"1999-09-10\",\"2000-09-09\"]] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 3 data frames with the above data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee = sc.parallelize(employee_body).toDF(employee_columns)\n",
    "job = sc.parallelize(job_body).toDF(job_columns)\n",
    "salary = sc.parallelize(salary_body).toDF(salary_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename the columns by using using capital letters and replace '_' with space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in employee.columns:\n",
    "    employee = employee.withColumnRenamed(col,col.replace(\"_\",\" \"))\n",
    "print(employee.columns)\n",
    "\n",
    "for col in job.columns:\n",
    "    job = job.withColumnRenamed(col,col.replace(\"_\",\" \"))\n",
    "print(job.columns)\n",
    "\n",
    "for col in salary.columns:\n",
    "    salary = salary.withColumnRenamed(col,col.replace(\"_\",\" \"))\n",
    "print(salary.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format birth_date as 01.Jan.2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, date_format\n",
    "\n",
    "employee = employee.withColumn(\"birth date\",date_format(col(\"birth date\"),\"d.MMM.y\"))\n",
    "employee.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a new column in employeeData where you compute the company email address by the\n",
    "following rule: [first 2 letter of first_name][last_name]@company.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, substring, lit\n",
    "\n",
    "_concat = (\n",
    "    concat(\n",
    "        substring(col(\"first name\"),0,2),\n",
    "        col(\"last name\"),\n",
    "        lit(\"@company.com\")\n",
    "    )\n",
    ")\n",
    "employee = employee.withColumn(\"email\", _concat)\n",
    "employee.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Calculate the average salary for each job role "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff, current_date, when\n",
    "\n",
    "join_table = salary.alias(\"salary\").join(other=job.alias(\"job\"), on=\"emp no\", how=\"inner\")\n",
    "join_table = join_table[[\n",
    "    \"salary.`emp no`\",\n",
    "    \"job.`title`\", \n",
    "    \"salary.`salary`\",\n",
    "    \"salary.`from date`\",\n",
    "    \"salary.`to date`\"\n",
    "]]\n",
    "\n",
    "# The approach is going to be first calculate the avg salary of each employee-role combination throughtout time so that we know \n",
    "# how much each person in each role earns on average, next we use that info to know the average salary of the role itself\n",
    "\n",
    "final = (\n",
    "    join_table\n",
    "    .withColumn(\n",
    "        \"years\",\n",
    "        when(col(\"to date\")==\"9999-01-01\",datediff(current_date(), col(\"from date\"))/365)\n",
    "        .otherwise(datediff(col(\"to date\"), col(\"from date\"))/365)\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"total_earned\",\n",
    "        col(\"years\")*col(\"salary\")\n",
    "    )\n",
    "    .groupBy(\n",
    "        col(\"emp no\"),\n",
    "        col(\"title\")\n",
    "    )\n",
    "    .agg(\n",
    "        (\n",
    "            F.sum(col(\"total_earned\"))/F.sum(col(\"years\"))\n",
    "        )\n",
    "        .alias(\"avg_per_employee_role\")\n",
    "    )\n",
    "    .groupBy(\n",
    "        col(\"title\")\n",
    "    )\n",
    "    .agg(\n",
    "        (\n",
    "            F.sum(col(\"avg_per_employee_role\"))/F.count(col(\"emp no\"))\n",
    "        ).alias(\"avg_per_role\")\n",
    "    )\n",
    ")\n",
    "\n",
    "final.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a flag (set value to True) in salaryData if the average salary of the person is lower than the  average salary for their job role "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window as W\n",
    "\n",
    "# The approach here is going to be the same as before but now we have to use window functions to avoid joins\n",
    "\n",
    "final = (\n",
    "    join_table # Making use of the join_table defined in the previous exercise\n",
    "    .withColumn(\n",
    "        \"years\",\n",
    "        when(col(\"to date\")==\"9999-01-01\",datediff(current_date(), col(\"from date\"))/365)\n",
    "        .otherwise(datediff(col(\"to date\"), col(\"from date\"))/365)\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"total_earned\",\n",
    "        col(\"years\")*col(\"salary\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"avg_per_employee_role\",\n",
    "        F.sum(col(\"total_earned\")).over(W.partitionBy(col(\"emp no\"), col(\"title\")))\n",
    "        /\n",
    "        F.sum(col(\"years\")).over(W.partitionBy(col(\"emp no\"), col(\"title\")))\n",
    "        \n",
    "    )\n",
    "    .withColumn(\n",
    "        \"avg_per_role\",\n",
    "        F.sum(col(\"avg_per_employee_role\")).over(W.partitionBy(col(\"title\")))\n",
    "        /\n",
    "        F.count(col(\"emp no\")).over(W.partitionBy(col(\"title\")))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"avg_per_employee\",\n",
    "        F.sum(col(\"total_earned\")).over(W.partitionBy(col(\"emp no\")))\n",
    "        /\n",
    "        F.sum(col(\"years\")).over(W.partitionBy(col(\"emp no\")))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"flag\",\n",
    "        when(col(\"avg_per_employee\")<col(\"avg_per_role\"),lit(True))\n",
    "        .otherwise(lit(False))\n",
    "    )\n",
    ")\n",
    "\n",
    "salary = final[[\"salary.*\",\"flag\"]]\n",
    "    \n",
    "salary.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "03009617935e6ac43046589562b0e2b6c968c25e0d6c65e7827925ecbfb1f8e9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
